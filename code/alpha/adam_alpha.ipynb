{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from pyDOE import lhs\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNs():\n",
    "    def __init__(self, X_u, u, X_f, layers):\n",
    "        # data\n",
    "        self.x_u = torch.tensor(\n",
    "            X_u[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_u = torch.tensor(\n",
    "            X_u[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.alpha_u = torch.tensor(\n",
    "            X_u[:, 2:3], requires_grad=True).float().to(device)\n",
    "        self.x_f = torch.tensor(\n",
    "            X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_f = torch.tensor(\n",
    "            X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.alpha_f = torch.tensor(\n",
    "            X_f[:, 2:3], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        # deep neurla networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "         # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "        self.iter = 0\n",
    "\n",
    "    def net_u(self, x, t, alpha):\n",
    "        u = self.dnn(torch.cat([x, t, alpha], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t, alpha):\n",
    "        u = self.net_u(x, t, alpha)\n",
    "\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        f = u_t - alpha * u_xx\n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        u_pred = self.net_u(self.x_u, self.t_u, self.alpha_u)\n",
    "        f_pred = self.net_f(self.x_f, self.t_f, self.alpha_f)\n",
    "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
    "        loss_f = torch.mean(f_pred ** 2)\n",
    "        \n",
    "        loss = loss_u + loss_f\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
    "            )\n",
    "        return loss\n",
    "\n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(nIter):\n",
    "            u_pred = self.net_u(self.x_u, self.t_u, self.alpha_u)\n",
    "            f_pred = self.net_f(self.x_f, self.t_f, self.alpha_f)\n",
    "            loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(\n",
    "                    'It: %d, Loss: %.3e' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(), \n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        alpha = torch.tensor(X[:, 2:3], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t, alpha)\n",
    "        f = self.net_f(x, t, alpha)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_i = 1.0\n",
    "T_t = 0.0\n",
    "T_b = 0.0\n",
    "\n",
    "N_u = 1000\n",
    "N_f = 10000\n",
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "N_t = 100\n",
    "N_x = 100\n",
    "N_a = 100\n",
    "t = np.linspace(0, 1, N_t)\n",
    "x = np.linspace(-1, 1, N_x)\n",
    "alpha = np.linspace(0.0, 1, N_a)\n",
    "\n",
    "X, T, ALPHA = np.meshgrid(x, t, alpha)\n",
    "X_star = np.hstack(\n",
    "    (X.flatten()[:,None], T.flatten()[:,None], ALPHA.flatten()[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    " # Left boundary, t=0\n",
    "xx1 = np.hstack(\n",
    "    (X[0:1,:,:].flatten()[:,None], \n",
    "    T[0:1,:,:].flatten()[:,None], \n",
    "    ALPHA[0:1,:,:].flatten()[:,None]))\n",
    "uu1 = np.ones_like(xx1) * T_i\n",
    "# Lower boundary, x=-1\n",
    "xx2 = np.hstack(\n",
    "    (X[:,0:1,:].flatten()[:,None], \n",
    "    T[:,0:1,:].flatten()[:,None], \n",
    "    ALPHA[:,0:1,:].flatten()[:,None]))\n",
    "uu2 = np.ones_like(xx2) * T_b\n",
    "# Upper boundary, x=1\n",
    "xx3 = np.hstack(\n",
    "    (X[:,-1:,:].flatten()[:,None], \n",
    "    T[:,-1:,:].flatten()[:,None], \n",
    "    ALPHA[:,-1:,:].flatten()[:,None]))\n",
    "uu3 = np.ones_like(xx3) * T_t\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub-lb)*lhs(3, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PINNs(X_u_train, u_train, X_f_train, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 6.222e-01\n",
      "Iter 100, Loss: 5.01203e-02, Loss_u: 4.67464e-02, Loss_f: 3.37395e-03\n",
      "Iter 200, Loss: 4.20953e-02, Loss_u: 3.86837e-02, Loss_f: 3.41154e-03\n",
      "Iter 300, Loss: 3.73478e-02, Loss_u: 3.20363e-02, Loss_f: 5.31150e-03\n",
      "Iter 400, Loss: 2.90038e-02, Loss_u: 2.39951e-02, Loss_f: 5.00864e-03\n",
      "Iter 500, Loss: 2.23930e-02, Loss_u: 1.79615e-02, Loss_f: 4.43145e-03\n",
      "Iter 600, Loss: 1.74639e-02, Loss_u: 1.35255e-02, Loss_f: 3.93842e-03\n",
      "Iter 700, Loss: 1.44193e-02, Loss_u: 1.09474e-02, Loss_f: 3.47184e-03\n",
      "Iter 800, Loss: 1.25867e-02, Loss_u: 9.54962e-03, Loss_f: 3.03706e-03\n",
      "Iter 900, Loss: 1.15702e-02, Loss_u: 8.97706e-03, Loss_f: 2.59316e-03\n",
      "Iter 1000, Loss: 1.07127e-02, Loss_u: 8.32801e-03, Loss_f: 2.38472e-03\n",
      "Iter 1100, Loss: 1.00278e-02, Loss_u: 7.64799e-03, Loss_f: 2.37978e-03\n",
      "Iter 1200, Loss: 9.53245e-03, Loss_u: 7.23666e-03, Loss_f: 2.29579e-03\n",
      "Iter 1300, Loss: 9.16280e-03, Loss_u: 6.97684e-03, Loss_f: 2.18596e-03\n",
      "Iter 1400, Loss: 8.90381e-03, Loss_u: 6.83994e-03, Loss_f: 2.06387e-03\n",
      "Iter 1500, Loss: 8.60635e-03, Loss_u: 6.62487e-03, Loss_f: 1.98149e-03\n",
      "Iter 1600, Loss: 8.24931e-03, Loss_u: 6.39910e-03, Loss_f: 1.85022e-03\n",
      "Iter 1700, Loss: 8.07162e-03, Loss_u: 6.30699e-03, Loss_f: 1.76463e-03\n",
      "Iter 1800, Loss: 7.80697e-03, Loss_u: 6.16311e-03, Loss_f: 1.64387e-03\n",
      "Iter 1900, Loss: 7.55665e-03, Loss_u: 5.96369e-03, Loss_f: 1.59296e-03\n",
      "Iter 2000, Loss: 7.36109e-03, Loss_u: 5.74050e-03, Loss_f: 1.62060e-03\n",
      "Iter 2100, Loss: 7.14176e-03, Loss_u: 5.52402e-03, Loss_f: 1.61775e-03\n",
      "Iter 2200, Loss: 6.96885e-03, Loss_u: 5.33842e-03, Loss_f: 1.63043e-03\n",
      "Iter 2300, Loss: 6.82577e-03, Loss_u: 5.21054e-03, Loss_f: 1.61524e-03\n",
      "Iter 2400, Loss: 6.64862e-03, Loss_u: 5.02572e-03, Loss_f: 1.62291e-03\n",
      "Iter 2500, Loss: 6.51640e-03, Loss_u: 4.90746e-03, Loss_f: 1.60894e-03\n",
      "Iter 2600, Loss: 6.37644e-03, Loss_u: 4.74925e-03, Loss_f: 1.62718e-03\n",
      "Iter 2700, Loss: 6.24659e-03, Loss_u: 4.62501e-03, Loss_f: 1.62158e-03\n",
      "Iter 2800, Loss: 6.15702e-03, Loss_u: 4.51968e-03, Loss_f: 1.63734e-03\n",
      "Iter 2900, Loss: 6.04598e-03, Loss_u: 4.39856e-03, Loss_f: 1.64742e-03\n",
      "Iter 3000, Loss: 5.94380e-03, Loss_u: 4.32805e-03, Loss_f: 1.61575e-03\n",
      "Iter 3100, Loss: 5.81455e-03, Loss_u: 4.30147e-03, Loss_f: 1.51309e-03\n",
      "Iter 3200, Loss: 5.73242e-03, Loss_u: 4.25838e-03, Loss_f: 1.47404e-03\n",
      "Iter 3300, Loss: 5.64812e-03, Loss_u: 4.17159e-03, Loss_f: 1.47653e-03\n",
      "Iter 3400, Loss: 5.57793e-03, Loss_u: 4.14733e-03, Loss_f: 1.43060e-03\n",
      "Iter 3500, Loss: 5.49434e-03, Loss_u: 4.07599e-03, Loss_f: 1.41835e-03\n",
      "Iter 3600, Loss: 5.45478e-03, Loss_u: 4.05197e-03, Loss_f: 1.40282e-03\n",
      "Iter 3700, Loss: 5.34268e-03, Loss_u: 4.03712e-03, Loss_f: 1.30556e-03\n",
      "Iter 3800, Loss: 5.27433e-03, Loss_u: 4.01332e-03, Loss_f: 1.26101e-03\n",
      "Iter 3900, Loss: 5.18498e-03, Loss_u: 3.99315e-03, Loss_f: 1.19183e-03\n",
      "Iter 4000, Loss: 5.11696e-03, Loss_u: 3.97386e-03, Loss_f: 1.14310e-03\n",
      "Iter 4100, Loss: 5.06097e-03, Loss_u: 3.92753e-03, Loss_f: 1.13345e-03\n",
      "Iter 4200, Loss: 5.01230e-03, Loss_u: 3.89008e-03, Loss_f: 1.12223e-03\n",
      "Iter 4300, Loss: 4.96096e-03, Loss_u: 3.85389e-03, Loss_f: 1.10706e-03\n",
      "Iter 4400, Loss: 4.92041e-03, Loss_u: 3.81781e-03, Loss_f: 1.10260e-03\n",
      "Iter 4500, Loss: 4.88884e-03, Loss_u: 3.79290e-03, Loss_f: 1.09594e-03\n",
      "Iter 4600, Loss: 4.86237e-03, Loss_u: 3.78076e-03, Loss_f: 1.08160e-03\n",
      "Iter 4700, Loss: 4.81462e-03, Loss_u: 3.76581e-03, Loss_f: 1.04882e-03\n",
      "Iter 4800, Loss: 4.78508e-03, Loss_u: 3.75251e-03, Loss_f: 1.03256e-03\n",
      "Iter 4900, Loss: 4.75432e-03, Loss_u: 3.72516e-03, Loss_f: 1.02916e-03\n",
      "Iter 5000, Loss: 4.71033e-03, Loss_u: 3.67900e-03, Loss_f: 1.03133e-03\n",
      "Iter 5100, Loss: 4.67640e-03, Loss_u: 3.64863e-03, Loss_f: 1.02777e-03\n",
      "Iter 5200, Loss: 4.63467e-03, Loss_u: 3.62876e-03, Loss_f: 1.00591e-03\n",
      "Iter 5300, Loss: 4.59738e-03, Loss_u: 3.59159e-03, Loss_f: 1.00579e-03\n",
      "Iter 5400, Loss: 4.55317e-03, Loss_u: 3.55839e-03, Loss_f: 9.94786e-04\n",
      "Iter 5500, Loss: 4.52750e-03, Loss_u: 3.52834e-03, Loss_f: 9.99164e-04\n",
      "Iter 5600, Loss: 4.49831e-03, Loss_u: 3.51727e-03, Loss_f: 9.81035e-04\n",
      "Iter 5700, Loss: 4.46211e-03, Loss_u: 3.49574e-03, Loss_f: 9.66375e-04\n",
      "Iter 5800, Loss: 4.43480e-03, Loss_u: 3.46658e-03, Loss_f: 9.68226e-04\n",
      "Iter 5900, Loss: 4.39912e-03, Loss_u: 3.46131e-03, Loss_f: 9.37804e-04\n",
      "Iter 6000, Loss: 4.36609e-03, Loss_u: 3.46332e-03, Loss_f: 9.02772e-04\n",
      "Iter 6100, Loss: 4.33963e-03, Loss_u: 3.43550e-03, Loss_f: 9.04129e-04\n",
      "Iter 6200, Loss: 4.30763e-03, Loss_u: 3.41923e-03, Loss_f: 8.88402e-04\n",
      "Iter 6300, Loss: 4.28425e-03, Loss_u: 3.39120e-03, Loss_f: 8.93054e-04\n",
      "Iter 6400, Loss: 4.26133e-03, Loss_u: 3.36405e-03, Loss_f: 8.97286e-04\n",
      "Iter 6500, Loss: 4.23495e-03, Loss_u: 3.32886e-03, Loss_f: 9.06090e-04\n",
      "Iter 6600, Loss: 4.20121e-03, Loss_u: 3.28542e-03, Loss_f: 9.15794e-04\n",
      "Iter 6700, Loss: 4.16315e-03, Loss_u: 3.25593e-03, Loss_f: 9.07220e-04\n",
      "Iter 6800, Loss: 4.12760e-03, Loss_u: 3.21171e-03, Loss_f: 9.15885e-04\n",
      "Iter 6900, Loss: 4.09824e-03, Loss_u: 3.18857e-03, Loss_f: 9.09673e-04\n",
      "Iter 7000, Loss: 4.07740e-03, Loss_u: 3.17754e-03, Loss_f: 8.99868e-04\n",
      "Iter 7100, Loss: 4.05498e-03, Loss_u: 3.16418e-03, Loss_f: 8.90808e-04\n",
      "Iter 7200, Loss: 4.03590e-03, Loss_u: 3.14642e-03, Loss_f: 8.89481e-04\n",
      "Iter 7300, Loss: 4.01403e-03, Loss_u: 3.13354e-03, Loss_f: 8.80492e-04\n",
      "Iter 7400, Loss: 3.98322e-03, Loss_u: 3.11643e-03, Loss_f: 8.66785e-04\n",
      "Iter 7500, Loss: 3.95732e-03, Loss_u: 3.10567e-03, Loss_f: 8.51651e-04\n",
      "Iter 7600, Loss: 3.93404e-03, Loss_u: 3.08825e-03, Loss_f: 8.45781e-04\n",
      "Iter 7700, Loss: 3.89763e-03, Loss_u: 3.06497e-03, Loss_f: 8.32666e-04\n",
      "Iter 7800, Loss: 3.87624e-03, Loss_u: 3.06059e-03, Loss_f: 8.15654e-04\n",
      "Iter 7900, Loss: 3.86181e-03, Loss_u: 3.04821e-03, Loss_f: 8.13602e-04\n",
      "Iter 8000, Loss: 3.84036e-03, Loss_u: 3.04134e-03, Loss_f: 7.99025e-04\n",
      "Iter 8100, Loss: 3.81743e-03, Loss_u: 3.02431e-03, Loss_f: 7.93125e-04\n",
      "Iter 8200, Loss: 3.78549e-03, Loss_u: 2.98924e-03, Loss_f: 7.96253e-04\n",
      "Iter 8300, Loss: 3.76635e-03, Loss_u: 2.97907e-03, Loss_f: 7.87285e-04\n",
      "Iter 8400, Loss: 3.73945e-03, Loss_u: 2.96250e-03, Loss_f: 7.76948e-04\n",
      "Iter 8500, Loss: 3.71741e-03, Loss_u: 2.96605e-03, Loss_f: 7.51356e-04\n",
      "Iter 8600, Loss: 3.69831e-03, Loss_u: 2.97095e-03, Loss_f: 7.27359e-04\n",
      "Iter 8700, Loss: 3.67556e-03, Loss_u: 2.94886e-03, Loss_f: 7.26707e-04\n",
      "Iter 8800, Loss: 3.65746e-03, Loss_u: 2.94040e-03, Loss_f: 7.17055e-04\n",
      "Iter 8900, Loss: 3.64072e-03, Loss_u: 2.92064e-03, Loss_f: 7.20076e-04\n",
      "Iter 9000, Loss: 3.62515e-03, Loss_u: 2.90699e-03, Loss_f: 7.18163e-04\n",
      "Iter 9100, Loss: 3.60911e-03, Loss_u: 2.90030e-03, Loss_f: 7.08809e-04\n",
      "Iter 9200, Loss: 3.59455e-03, Loss_u: 2.87264e-03, Loss_f: 7.21919e-04\n",
      "Iter 9300, Loss: 3.57978e-03, Loss_u: 2.84850e-03, Loss_f: 7.31273e-04\n",
      "Iter 9400, Loss: 3.56632e-03, Loss_u: 2.84202e-03, Loss_f: 7.24299e-04\n",
      "Iter 9500, Loss: 3.54986e-03, Loss_u: 2.82909e-03, Loss_f: 7.20769e-04\n",
      "Iter 9600, Loss: 3.52982e-03, Loss_u: 2.79850e-03, Loss_f: 7.31313e-04\n",
      "Iter 9700, Loss: 3.51025e-03, Loss_u: 2.78136e-03, Loss_f: 7.28892e-04\n",
      "Iter 9800, Loss: 3.49198e-03, Loss_u: 2.76654e-03, Loss_f: 7.25445e-04\n",
      "Iter 9900, Loss: 3.47591e-03, Loss_u: 2.76053e-03, Loss_f: 7.15386e-04\n",
      "Iter 10000, Loss: 3.46149e-03, Loss_u: 2.74991e-03, Loss_f: 7.11573e-04\n",
      "Iter 10100, Loss: 3.44404e-03, Loss_u: 2.72922e-03, Loss_f: 7.14816e-04\n",
      "Iter 10200, Loss: 3.42847e-03, Loss_u: 2.72534e-03, Loss_f: 7.03129e-04\n",
      "Iter 10300, Loss: 3.40792e-03, Loss_u: 2.69877e-03, Loss_f: 7.09148e-04\n",
      "Iter 10400, Loss: 3.38454e-03, Loss_u: 2.68058e-03, Loss_f: 7.03952e-04\n",
      "Iter 10500, Loss: 3.36754e-03, Loss_u: 2.67398e-03, Loss_f: 6.93558e-04\n",
      "Iter 10600, Loss: 3.35446e-03, Loss_u: 2.65071e-03, Loss_f: 7.03755e-04\n",
      "Iter 10700, Loss: 3.33960e-03, Loss_u: 2.64500e-03, Loss_f: 6.94602e-04\n",
      "Iter 10800, Loss: 3.32530e-03, Loss_u: 2.63668e-03, Loss_f: 6.88624e-04\n",
      "Iter 10900, Loss: 3.31087e-03, Loss_u: 2.63110e-03, Loss_f: 6.79774e-04\n",
      "Iter 11000, Loss: 3.29873e-03, Loss_u: 2.61432e-03, Loss_f: 6.84411e-04\n",
      "CPU times: user 15min 44s, sys: 1.03 s, total: 15min 45s\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff95488729f9cc551556abf022528fbc80f7f79ac92553f8f6ed3550998a7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
